Sat Oct 19 05:41:54 JST 2013

index
---
* 線形分離: パーセプトロン
* ニューラルネットワーク

ニューラルネットワークについて
ニューラルネットワークは多層パーセプトロンと呼ばれることもあり
パーセプトロンを層として並べるように置くのである．
ただし使い方はパーセプトロンよりも高級である


線形分離: パーセプトロン
---
ユニットとは，入ってくるいくつかの矢印と，出ていく矢印の付いた，○です．
入ってくる矢印がn本だとして，それぞれには 重み (実数. 大抵 正の実数) がついている
上から順に
  w1, w2, ..., wn
とします．

で，そもそも線形分離とは
nコの入力 x1, x2, ..., xn と1コの出力 y <- {-1, 1} を計算する未知の f がある時

  f :: (R, R, ..., R) -> {-1, 1}
  f (x1, x2, ..., xn) = y

この f を再現する何かを構成すること．線形分離は線形で f を再現しようとする．そもそも
解けないかもしれないけど解けるかもしれない．

  y' = if (w1*x1 + w2*x2 + ... + wn*xn + b) < 0 then -1 else 1

これが線形分離．つまり，入力の数だけ用意した何か重み w と，バイアス b (これも実数) とで
入力との線形結合をなして，それの 正負 を出力に対応させる．

機械学習ではよく

  sgn x = if x < 0 then -1 else 1

が使われる．数学的には sgn とは引数が正なら1，負なら-1を返し，引数がゼロならゼロを返すけど
今の場合は，正の時とゼロの時に1を返す．まあそんな細かいことはどっちでもいいんだけどね．

この sgn は Heaviside function だったり，活性化関数 (activation function) と呼ばれたりする．
正直どうでもいい．
ゼロを中心に点対称 (奇関数) っぽくみえて，出力がコドメインに収まってさえすれば

幾何的に意味を与えると，入力をn次元座標上の点だとして，ある超平面よりその点が上にあるか下にあるか
で出力を与える．とかなんとか．正直そんな幾何的な意味は頭にあっても何の役にもたちゃあしない．
むしろ思考に制限を与えてしまうだけだ．

これを機能サせるためには当然ながら，
  w1, w2, ..., wn, b
という n+1 コの実数の値をどこかから持ってこないといけない．
そこで 訓練 (train) という過程が必要になる．

沢山の
  (x1 .. xn ; y)
という組 (訓練用データ) をどこかから持ってきて
これらから (w1 .. wn; b) を学習すればよいのだ．
そのアルゴリズムが単純パーセプトロン

最初に適当な，例えばゼロという定数でも良いので w1 .. wn; b に数値を割り当てる
先程の訓練用データを観て，もっと大きいほうがいいか小さいほうがいいかを観て，それならちょっと大きくする，
ちょっと小さくする，を全ての訓練用データで正しく判別できるまで繰り返すだけ

その「ちょっと」を調整するために，適当な１より小さい正の実数 mu を用いる

> mu = 0.01
> 
> perceptron (x1 .. xn) (w1 .. wn; b) = x1*w1 + .. + xn*wn + b
> 
> update (x1 .. xn; y) (w1 .. wn; b) =
>   if y * (perceptron (x1 .. xn) (w1 .. wn; b)) > 0
>     then (w1 .. wn; b)
>     else (w1 .. wn; b) + (x1 .. xn; 1) * mu * y

出力を0, 1 ではなく-1, 1 とした利点は，正解 y <- {-1, 1} に対して予測値 y' <- {-1, 1} があった時
> y * y'
これが正なら予測が正しいことを意味する，なんていう判定の仕方ができるからだと思うよ
いや，まあ，普通に
> y == y'
でいいんだけどさ
複数の本でよくこういう判定の仕方してるから，さ．



ニューラルネットワーク
---
層を考える
層は３つあればいいらしい．
一つ目の層にはnコの入力が並んでる．
２つ目の層には，Mコの○が並んでる．この○とは一番最初に説明しました．
それぞれ一つの○にはn本，入ってくる矢印があります．nというのは入力に対応してる．
第一層と第二層は， n * M 本の矢印でつながっています．

第三層は出力がkコ並んでる．出力が複数あるような問題は考えられるけど，k=1 とします．
つまり，第三層にはただひとつの○がある
この○は入ってくる M コの矢印があり，出ていく矢印が出力そのもの．

一つの○はそれぞれ独自の
  (w1 .. wn; b)
を持っており，
入力 (x1 .. xn) に対して

  y' = f (w1*x1 + w2*x2 + ... + wn*xn + b)

これを出力する
この f とは，先程 sgn を使っていたやつ．一般にこれを活性化関数という．
つまり，実数全体を，{-1, 1} (あるいは {0, 1}) に移す．
-1 か 1 のどちらか(あるいは 0 か 1 のどちらか) ，と言いたいけど，連続関数である方が微分が使えて都合がよいので，

第二層の○の活性化関数は tanh を使う
第三層の○の活性化関数には sigmoid を使う

tanh は tanh. sigmoid は私はこんなの聞いたことないけど定義は

> sigmoid x = 1 / (1 + exp (- x))

sigmoid (大きい正数) = 1
sigmoid (大きい負数) = 0
sigmoid 0 = 0.5

sigmoidは大体0か1かに分けます
従って，ニューラルネットワーク全体の出力は [0, 1] という実数範囲．
最終的に，これを使うときには 四捨五入とかして整数にキャストする


で，学習方法だけど，疲れたので

/js/neuralNetwork.js.txt

を参照

