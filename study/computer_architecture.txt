計算機構成法

教科書;
ヘネシー＆パターソン
コンピュータアーキテクチャ　定量的アプローチ　第四版

#分岐予測
分岐命令はパイプライン実行の性能を低下させ、また
命令のプリフェッチのハザードにもなる．分岐が成立するか
否かを予測することで命令をプリフェッチ、投機的実行(後で
取り消すかもしれないの前提で実行してしまうこと)をする
ことで全体的に見れば計算速度が向上することを期待する．

静的分岐予測．コンパイル時に分岐を予測するもの．最も
単純なのは常に成立すると予測することだろう．しかし平均
すればミス率は50%程度で、またプログラムによってどちらか
に集中するというのが一般的．それを利用して、前回の実行時
にプロファイル情報を作成して、次のコンパイル時に活かす．

最近のプロセッサとしては静的ではなく、次の動的分岐予測
をするのが主流である．

簡単な動的分岐予測は分岐予測バッファを利用する．これは
分岐命令のアドレス(の下位ビット)をインデックスとする
テーブルであり、その分岐命令が前回は成立したか否かの
履歴を反映させた状態としての値を書いておくものである．

1bit分岐予測と呼ばれるのは、単純に前回成立したらその旨を
書いておき、次も成立すると予測する．前回不成立だったなら
次も不成立と予測する．
もしも途中に一度だけ不成立するような分岐を沢山実行され
たなら、まず不成立の時に一度予測を外し、次に予測を外す
ために二度目の予測を外すことになる．

2bit分岐予測は1bitの強化版であり、分岐予測バッファには
それぞれの分岐命令に対して4つの状態を書く．

状態00 -- 成立すると予測
状態01 -- たぶん、成立と予測
状態10 -- たぶん、不成立と予測
状態11 -- 不成立と予測

成立、不成立によって次のように状態を遷移させる．
成立 -- 01->00, 00->00, 11->10, 10->00
不成 -- 00->01, 01->11, 11->11, 10->11

  成 <=> (成)
 /        /
(不) <=> 不

二度続けて成立、または不成立したら予測をそのように変えるものである．

このようにバッファのサイズを大きくすることで予測精度は改善され、
実際2bitあれば十分であるらしい．

#ハザード
パイプラインすると起こる危機．

命令実行の流れの中で何か次の命令の実行を妨げるような情況のことを
ハザードといい、大きく三つに分類される．

1. 構造ハザードは考えられるあらゆる命令の組み合わせでオーバーラップ
させて実行できるようにハードウェアがなっていない場合におこる
資源の衝突．
2. データハザードは前の命令とその次の命令との間に依存関係がある場合．
パイプライン実行で命令がオーバーラップして実行される為に起こる．
3. 制御ハザードは分岐命令など、program counterを変える命令を
パイプライン処理する為に起こる状態．

構造ハザード．
パイプライン処理をするならば資源は複製しなければならない．
例えば、レジスタ書き込みポートが一つしかないプロセッサは、
1クロック内で2回書き込みを要求されると構造ハザードを起こす．
構造ハザードが実際に起きたらどうするか．要求されたユニット
が利用可能になるまで命令の実行を待たせる．即ちストールさせる．

資源を十分な数だけ複製すれば単純にハザードは起こらなくなる．

#キャッシュ

高速なメモリ程高価であり、経済的な解決として、いくつかの層
によってメモリを構成する．上の層ほど高速であるが、容量は
小さい．レジスタが最も高速なめもりだ．下の層ほど低速だが、
容量は大きい．HDDなんかは安く数TBのものがある．SSDは速い
けどまだ高いし、容量も小さい．
(wikipediaを見てみたらHDDより下に「ネットワークサーバ」
があった．なるほどと思った)

上の層でキャッシュを作り、一度下の層(メインメモリ)から
取ってきたデータを保持しておく．そうすれば再びそのデータ
にアクセスする時、高速だ．しかし当然ながら容量はメイン
メモリよりずっと小さいから、全てを持つわけにはいかないし、
またただ一度アクセスしたデータのみを持っていても、そこまで
効果はない．これには空間的局所性と時間的局所性を利用する．

一度、キャッシュには存在しないメインメモリ上のデータにアクセス
されたら、そのデータを含む複数のワードを一辺に、これをブロック
という単位で呼ぶことにするが、ブロックをキャッシュに記憶させる
ことにする．そしてキャッシュのどこに記憶させるか．キャッシュは
Nコのセットからなると考えよう．キャッシュをNコに分割したと
考えてもよい．
     i = (address of the block) `mod` N
として、セットのiコ目の中の自由なところにブロックを置くことに
する．セットの中にいくつブロックが置けるようにするかは、
"n-way set-associative"と言う．二つなら"2way ..."である．

1セットに1ブロックしかないようなもの、そのようなキャッシュでは
セットという概念は最早意味を持たないが、そのブロックはいつも
特定の場所に配置されるようなキャッシュを direct map cach という．

キャッシュミス、つまり欲しいデータがキャッシュに無いという
状況は三つに分類されて

1. 初期ミス(Compulsory)
    最初にアクセスするデータはキャッシュにあるわけがない．
2. 容量ミス(Capacity)
    キャッシュ必要なデータを全て保持できないならば、
    いくつかのブロックは捨てられ、そして後で再び取ってくる
    ことになるかもしれない．
3. 競合ミス(Conflict)
    キャッシュにおけるブロックの配置が古アソシアティブで
    無ければ競合する．そしてそこにあったキャッシュは捨て
    られ、後でまた取ってくるかもしれない．

キャッシュミスはどうすれば減らせるか．
最も単純な方法は、ブロックサイズを増やすことである．
空間的局所性が利用されれば初期ミスは減らされる．
キャッシュ全体を大きくする．これも当然だ．
ウェイ数、即ちセットの中のブロック数を増やす．
これは競合ミスの低減につながる．
複数レベルのキャッシュを用いる．
1次キャッシュ(L1$)、2次キャッシュ(L2$)、…、というやつだ．

キャッシュミスの確率をPm
ヒット時のアクセス時間をTh
ミス時のアクセス時間をTm
とすると、
データのアクセス時間は期待値で
    Th * (1 - Pm) + Tm * Pm
と書ける．

Thを減らす為に、TLB(Tlanslation Lookahead buffer)を用いる方法
がある．キャッシュはプロセサが要求する仮想アドレスを物理アドレス
に変換する必要がある．仮想アドレスの一部をインデックスとして
扱うテーブルをキャッシュの一つの機構として持っておく．

ノンブロッキングキャッシュとは、キャッシュミスでデータを転送して
る間に次の命令をストールせずに実行してしまうようなもののこと．
さらにその間に次の命令がキャッシュヒットが可能であるもの．これは
Tmを減らして振舞う．

プロセッサは大抵、キャッシュのブロックの中の一つのワードしか
必要としないという観察がある．ここで"重要ワード優先 (critical
word first)"と"早期実行再開 (early restart)" という二つの方法
が思いつく．つまり、ミスしたワードだけを最初にメインメモリから
持ってき、残りのブロックが転送されてる間にも実行を再開する
のが重要ワード優先であり、また、早期実行再開とは、ブロックは
通常通りの順番で持ってき、必要なそれが来たらそれをプロセッサに
送って実行を再開するもの．残りの転送は非同期に行う．

メモリの読み込みでは、キャッシュはただのコピーである．書き込みの
場合、キャッシュのデータとメモリのデータの一貫性を保たねば
ならない．write-back cach という方針では、コピーであるキャッシュ
の上の値だけを書き換える．ブロックが置き換わるとき、つまりその
キャッシュから追い出される時、メインメモリも書き換える．対して
through cach は、キャッシュ上を書き換えたら即座にメインメモリ上
でも書き換える．

どちらの方式であっても、write buffer を利用するのがよい．
つまり、メインメモリの上を書き換える時、その書き換えをプロセサ
はいちいち待つのではなく、近くにあるwrite bufferと呼ばれる
領域にさっさと書いてしまい、次の命令の実行を再開する．
バッファは勝手に非同期にメインメモリへの書き込みを行う．

write bufferにおけるデータにもキャッシュにおけるブロックという
単位を持たせる．つまり、あるブロックに属するワードが今ライト
バッファに含まれている．次に同じブロックに属する別のワードを
ライトバッファに送る時、バッファの中で同じエントリに持たせて
おく．そして実際にメインメモリに書きこむ時、一辺に行う．
1ワードをn回書きこむよりもnワードを1回書く方が高速だからである．
この方式をwrite merge という．


キャッシュに複数のレベルを持たせる．つまり、例えば3つのレベルと
すると、1次キャッシュ(L1)から3次(L3)までがあって、容量は、
L1 < L2 < L3 となっている．一般的にどのように使うか．まずプロセサ
がメモリアクセスを要求してきたら、キャッシュから探すのだが、
順番はL1が最初で最後にL3を探して無ければメインメモリに行く．
メインメモリから持ってきたデータはL3に持っておく．キャッシュヒット
したデータは一つ上の、つまりL3でヒットしたらL2に、L2でヒットしたら
L1に持ち上げる．(この時データをコピーするだけである)

対してvictim cach、犠牲者キャッシュとはまた別の使い方をする．
キャッシュの中の捜索はやはりL1から順に行うが、ミス時、データは
いきなりL1に置く．そして、キャッシュからデータが溢れた時、
データは一つ下のキャッシュに送る．つまりL1からはL2、L2からはL3
におく．

これより前のキャッシュでは、L1のデータはL2にも存在することに
なっていた．包含関係が書ける．一方victim cachはnon-inclutionの
関係にある．必ずしもexclusionではないが、キャッシュの容量が
あまり大きくない時に有効である．
アドレスのインデクスが偶然重複した場合に近い昔にアクセスした
ブロックでも追いだしてしまうが、victim cachでは時間的局所性
が保たれる．

二次キャッシュが一次よりもヒット率が低い理由
メモリ参照の局所性に従うアクセスのほとんどは一次キャッシュに
ヒットし、二次キャッシュへのヒットは局所性から外れたランダム
アクセスであるため．

オペランド数で分類する命令の例
0 .. pop, push
1 .. clr r0 ; r0 <- 0
     jmp r0 ; goto r0
2 .. add r1 r2 ; r1 <- r1 + r2
3 .. add r1 r2 r3 ; r1 <- r2 + r3

パイプラインに適したオペランド数
3が一番データハザードが回避できるそうだ．

オペランド数を増やすと発生する問題点とその改善方法
命令長が長くなり、デコードが忙しくなる．
アドレシングモードを限定し、オペランド指定の命令長を短くするとか．

