# 自動要約の導入

元ネタ: http://www.slideshare.net/hitoshin/automatic-summarization

## 目的

長い文章から短い文章を生成すること

## 要約の種類

- 指示的要約: もとの文章を読むべきか(要約結果に加えるかどうか)判断する要約
- 報知的要約: もとの文章の代替となる文章を作る要約

- 単一文要約: 一つの文章から一つの文を選んで要約とする
- 複数文要約: 一つの文章から複数の分を選んで要約とする

- 抽出的要約: 元の文章の断片を繋ぎ合わせる要約
- 生成的要約: そうじゃなくて新しく生成する

## 必要な技術

- 文の分割
    + 逐次予測
- 文の短縮: 修飾節は削除するとか
    + 構文木枝刈
- 最重要抽出: 要約に相応する文を選択する
    + 最大被覆、ナップサック
- 順序: 複数文章が入力の時、並び替えがあるといい
    + セールスマン

## 評価

- 要約の質
- 文章の質: 文の自然さ

### ROUGE

元と要約文について、N-gram 類似度をみる

### 他

1. 文法の適切さ
1. 内容の繰り返しがないこと
1. 先行しない指示語がないこと
1. 無関係な情報がないこと
1. 接続関係が正しい

今のところ、評価を自動で行うような手法はとくにない

## 文選択

### ナップサック

単一文章に適用できる．
文章の長さを限定して、スコアを最大化する

### 最大被覆

複数文章に適用する．
できるだけ多くの単語を文章に含ませたい

話題を漏れ無くカバーする．
重複もできるだけ減らしたい．

### 施設配置

大橋さん解説だと、

N個、文を持ってくる．
ここからできるだけ少ないm個選んで、全ての話題を網羅したい．
話題の含意関係（交差関係？）で枝を張って、施設配置する．

## 並び替え

### セールスマン

文のつながりにコストを考えてそれを最小化する．
コストは学習する？


# Subtree extractive summarization via submodular

[Subtree Extractive Summarization via Submodular Maximization - P13-1101.pdf](http://www.newdesign.aclweb.org/anthology-new/P/P13/P13-1101.pdf)

## 劣モジュラーとは

次を満たす `f`

```haskell
f A + f B >= f (A and B) + f (A or B)
```

貪欲法の性能保証に使われる。
今回は文選択のその貪欲に使われている。

## 手法

JUMAN のような、dependency parser で出来た構文木の、
根から始る全ての部分木は一つの文を表している。

つまり、普通、要約において文の選択といえば、
選択された文は元の文章における文と変わっていないけど、
今回は構文の正しさは保証されたレベルで、文をさらに小さくする。

部分木の集合から、適当な評価関数を用いて選択してく。
元々同じ文(木)からできた部分木が重複して選ばれた場合、
自然にマージする。

## 評価関数

- クエリとの関連性
- 長さ (短すぎては困る)

の線形結合


# Baysean with background knowledge

## 関連

誰でも知ってる知識の要約なんていらない。
新規性のある文だけを、スコア `surprise` を附けて抽出する。

そのために、誰でも知ってる知識であるところの
「仮説の事前分布」
を構築する。
それは、単語の出現分布であるとする。

ただし、どんな単語についてでも考えてたらしょうがないので、
`topic word`
を考える

## topic word

帰無仮説

> H<sub>0</sub>: word `t` が、`I` でも `B` でも同じ確率で出現する

ここで

- `I` とは Input corpus (新しい文章)
- `B` とは Background corpus (事前知識の文章)

この仮説が棄却されたら、word `t` は `topic word` だとする。

### 単語出現の分布をディレクレ分布だとする

語彙 `w1 .. wV` を考えるとき、
それぞれの出現回数 `a1 .. aV` の出現回数は

`Dir(a1 .. aV)`

に従うとする。

これが事前知識だとすると、
`c1 .. cV`
が新しい知識として取り込むとき

`Dir(a1 + c1 .. aV + cV)`

として簡単に取り込めるので都合が良い。

それぞれを、事前確率分布、事後確率分布だと考える。

## surprise

`surprise` は
事前、事後の確率分布の距離を見ればよい。
確率分布の距離は普通 KL-divergence とかを使うだろう。

さて実際には、
一文に対して、含まれる単語ずつ、単語の`surprise` を計算する。
文の`surprise` を、単語の `surprise` の平均とする

んで、文を選択していく



