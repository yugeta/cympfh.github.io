#
Using
Maximum
Entropy
for
Text
Classification
1999
年
論文
線形
SVM
??
##
最大
エントロピー
テキスト
分類
直感
的
知見
class
A
,
B
,
C
,
D
4
文書
とき
次
命題
>
"
professor
"
単語
文書
40
%
class
A
ここ
次
よう
確率
推定
の
自然
-
professor
文書
40
%
確率
class
A
.
各々
20
%
確率
class
B
,
C
,
D
.
-
professor
文書
25
%
確率
class
A
..
D
##
model
###
Maximum
entropy
-
d
:
document
-
c
:
class
素性
$
f
_
i
(
d
,
c
)$
これ
関数
具体
的
$
f
_
i
$
単語
`
w
`
カウント
コーパス
`
D
`
コーパス
`
D
`
文章
`
d
`
集合
`
d
`
class
`
c
=
c
(
d
)`
これ
素性
$
f
_
i
$
平均
1
.
$$\
dfrac
{
1
}{|
D
|}
\
sum
_{
d
\
in
D
}
f
_
i
(
d
,
c
(
d
))$$
分布
`
P
(
d
)`,
`
P
(
c
|
d
)`
`
d
`
`
f
_
i
`
期待
値
$$\
sum
_
c
P
(
c
|
d
)
f
_
i
(
d
,
c
)$$
コーパス
`
D
`
期待
値
2
.
$$\
sum
_{
d
\
in
D
}
P
(
d
)
\
sum
_
c
P
(
c
|
d
)
f
_
i
(
d
,
c
)$$
(
1
.)
(
2
.)
こと
最大
エントロピー
法
仮定
$$
P
(
d
)
=
1
/
|
D
|$$
$$\
sum
_{
d
\
in
D
}
f
_
i
(
d
,
c
(
d
))
=
\
sum
_{
d
\
in
D
}
\
sum
_
c
P
(
c
|
d
)
f
_
i
(
d
,
c
)$$
Della
Pietra
+,
1997
最大
エントロピー
確率
分布
exp
形
今
場合
素性
列
$$[
f
_
1
,
f
_
2
,
\
dots
,
f
_
n
]$$
これら
対応
重み
$$\
Lambda
=
[
\
lambda
_
1
,
\
lambda
_
2
,
\
dots
,
\
lambda
_
n
]
$$
存在
次
よう
$$
P
(
c
|
d
)
=
\
frac
{
1
}{
Z
(
d
)}
\
exp
\
left
[
\
sum
_
i
\
lambda
_
i
f
_
i
(
d
,
c
)
\
right
]$$
これ
対数
尤
度
$$\
ell
(\
Lambda
,
D
)
=
\
log
\
prod
_{
d
\
in
D
}
P
_\
Lambda
(
c
(
d
)
|
d
)$$
###
Improved
Iterative
Scaling
(
IIS
)
先
$$\
ell
(\
Lambda
,
D
)
=
\
log
\
prod
_{
d
\
in
D
}
P
_\
Lambda
(
c
(
d
)
|
d
)$$
よう
$\
Lambda
$
調整
山登り
法
これ
$$\
frac
{\
partial
^
2
}{\
partial
\
lambda
_
i
^
2
}
\
ell
$$
上
凸
山登り
OK
.
$$\
Lambda
\
rightarrow
\
Lambda
+
\
Delta
where
\
Delta
=
[
\
delta
_
1
...
\
delta
_
n
]$$
山登り
$$\
ell
(\
Lambda
+
\
Delta
|
D
)
-
\
ell
(\
Lambda
|
D
)
=
\
sum
_
d
\
sum
_
i
\
delta
_
i
f
_
i
-
\
sum
_
d
\
left
[
\
log
\
sum
_
c
\
exp
\
sum
_
i
\
left
[
(\
lambda
_
i
+
\
delta
_
i
)
f
_
i
\
right
]
-
\
log
\
sum
_
c
\
exp
\
left
[
\
sum
_
i
\
lambda
_
i
f
_
i
\
right
]
\
right
]$$
$$
it
=
\
sum
_
d
\
sum
_
i
\
delta
_
i
f
_
i
-
\
sum
_
d
\
log
\
dfrac
{\
sum
_
c
\
exp
\
sum
(\
lambda
+
\
delta
)
f
}{\
sum
_
c
\
exp
\
sum
_
i
\
lambda
f
}$$
This
derived
as
follows
because
Jensen
'
s
ineq
.
$$
it
\
ge
\
sum
_
d
\
sum
_
i
\
delta
_
i
f
_
i
-
\
log
\
sum
_
d
\
dfrac
{\
sum
_
c
\
exp
\
sum
(\
lambda
+
\
delta
)
f
}{\
sum
_
c
\
exp
\
sum
_
i
\
lambda
f
}$$
And
,
note
that
`-
log
x
>=
1
-
x
`
$$
it
\
ge
1
+
\
sum
_
d
\
sum
_
i
\
delta
_
i
f
_
i
-
\
sum
_
d
\
dfrac
{\
sum
_
c
\
exp
\
sum
(\
lambda
+
\
delta
)
f
}{\
sum
_
c
\
exp
\
sum
_
i
\
lambda
f
}
=
B
$$
This
`
B
`
is
a
function
of
`
Lambda
`.
式
綺麗
$$
B
=
1
+
\
sum
_
d
\
sum
_
i
\
delta
_
i
f
_
i
-
\
sum
_
d
\
dfrac
{\
sum
_
c
\
exp
\
sum
(\
lambda
+
\
delta
)
f
}{
Z
(
d
)}$$
$$
f
^\#
=
f
^\#(
d
,
c
)
=
\
sum
_
i
f
_
i
(
d
,
c
)$$
$$
B
=
1
+
\
sum
_
d
\
sum
_
i
\
delta
_
i
f
_
i
-
\
sum
_
d
\
dfrac
{\
sum
_
c
\
exp
\
left
[
\
sum
_
i
\
lambda
_
i
f
_
i
\
right
]
\
cdot
\
exp
\
left
[
\
sum
_
i
\
delta
_
i
f
_
i
\
right
]}{
Z
(
d
)}$$
$$
B
=
1
+
\
sum
_
d
\
sum
_
i
\
delta
_
i
f
_
i
-
\
sum
_
d
\
sum
_
c
P
_\
Lambda
(
c
|
d
)
\
exp
\
left
[
f
^\#(
d
,
c
)
\
sum
_
i
\
frac
{\
delta
_
i
f
_
i
(
d
,
c
)}{
f
^\#(
d
,
c
)}
\
right
]$$
>
ここ
論文
式
誤り
これ
次
偏
微分
$$\
frac
{\
partial
B
}{\
partial
\
delta
_
i
}
=
\
sum
_
d
\
left
[
f
_
i
(
d
,
c
(
d
))
-
\
sum
_
c
P
_\
Lambda
(
c
|
d
)
f
_
i
(
d
,
c
)
\
exp
\
left
[
\
delta
_
i
f
^\#(
d
,
c
)
\
right
]
\
right
]$$
これ
0
よう
`
delta
`
Newton
'
s
凸
性
解
こと
```
coffee
for
i
in
I
lambda
[
i
]
=
0
until
converge
#
or
step
for
i
in
I
solve
the
partial
equation
of
delta
[
i
]
lambda
[
i
]
+=
delta
[
i
]
```
###
Gaussian
Prior
(
事前
分布
?)
訓練
データ
とき
先
$\
lambda
_
i
$
もの
もの
誤差
よう
ガウス
分布
列
Lambda
自体
確率
次
よう
$$
P
(\
Lambda
)
=
\
prod
_
i
\
frac
{
1
}{\
sqrt
{
2
\
pi
\
sigma
_
i
^
2
}}
\
exp
\
left
[
\
frac
{-
\
lambda
_
i
^
2
}{
2
\
sigma
_
i
^
2
}
\
right
]$$
尤
度
これ
かけ算
対数
偏
微分
結局
次
1
項
足し算
$$\
frac
{\
lambda
_
i
+\
delta
_
i
}{-\
sigma
_
i
^
2
}$$
##
features
for
Text
Classification
for
a
word
`
w
`
and
class
`
k
`
```
haskell
f
_
i
d
c
|
c
==
k
=
N
(
d
,
w
)
/
N
d
|
otherwise
=
0
where
N
(
d
,
w
)
=
count
the
words
w
in
document
d
N
d
=
count
any
words
in
document
d
```
##
Experiment
-
Naiive
Bayse
(
comparison
)
-
Maximum
Entropy
(
w
/
o
Gaussian
Prior
)
-
Maximum
Entropy
(
w
/
Gaussian
Prior
)
場合
とき
