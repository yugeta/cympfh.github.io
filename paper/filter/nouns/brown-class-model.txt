#
Class
-
based
n
-
gram
models
of
natural
language
Brown
+
ら
[
Learning
phrase
patterns
for
Text
Classification
](
memo
/
learning
-
phrase
-
patterns
.
md
)
中
>
単語
クラス
1
次
マルコフ
モデル
尤
度
最大
化
こと
自動
分類
引用
#
Introduction
noisy
channel
経由
英語
文章
元
これ
一
議論
関与
こと
単語
クラス
こと
統計
的
これ
二
議論
#
言語
モデル
次
よう
言語
モデル
English
text
語
列
```
python
w
[
1
:
k
]
```
これ
条件
付き
確率
```
python
Pr
(
w
[
k
]
|
w
[
1
:
k
-
1
])
```
特徴
文章
全体
確率
$
Pr
(
w
[
1
:
k
])
=
Pr
(
w
[
1
])
\
cdot
Pr
(
w
[
2
]
|
w
[
1
:
1
])
\
cdot
Pr
(
w
[
3
]
|
w
[
1
:
2
])
\
cdots
Pr
(
w
[
k
]
|
w
[
1
:
k
-
1
])$
<
blockquote
>
python
-
like
つもり
```
python
w
[
i
:
j
]
```
列
```
tex
{
w
_
i
,
w
_{
i
+
1
},
...,
w
_
j
}
```
ここ
`
i
<=
j
`
暗黙
的
前提
</
blockquote
>
意味
`
w
[
1
:
k
-
1
]`
history
`
w
[
k
]`
prediction
##
n
-
gram
n
-
gram
language
model
history
内
最後
`(
n
-
1
)`
words
それ
history
做
```
python
Pr
(
w
[
k
]
|
w
[
1
:
k
-
1
])
=
Pr
(
w
[
k
]
|
w
[
k
-
n
+
1
:
k
-
1
])
```
`
k
>=
n
`
仮定
場合
確率
特別
2
-
gram
model
history
`
V
(
V
-
1
)`
通り
(
V
=
size
of
vocabulary
)．
それ
`
Pr
(
w
[
2
]
|
w
[
1
])`
`
V
-
1
`
通り
それら
確率
どっか
ら
training
text
尤
推定
割合
こと
`
C
(
w
)`
training
text
`
w
`
頻度
数
$
Pr
(
w
[
n
]
|
w
[
1
:
n
-
1
])
=
\
dfrac
{
C
(
w
[
1
:
n
-
1
]
w
[
n
])}{\
sum
_
w
C
([
1
:
n
-
1
]
w
)}$
ここ
`
w
[
1
:
n
-
1
]
w
`
列
末尾
word
一つ
追加
列
意味
##
interpolated
estimation
(
Jelinek
and
Mercer
,
1980
)
vocabulary
n
-
gram
`
n
`
指数
的
頻度
単純
`
n
`
ほう
モデル
精度
固定
語彙
信頼
性
interpolated
estimation
もの
いくつ
言語
モデル
$
Pr
^{(
j
)}$
構築
それら
combine
こと
$
Pr
'$
$$
Pr
'(
w
[
i
]
|
w
[
1
:
i
-
1
])
=
\
sum
_
j
\
lambda
_
j
(
w
[
1
:
i
-
1
])
Pr
^{(
j
)}(
w
[
i
]
|
w
[
1
:
i
-
1
])$$
重み
$\
lambda
_
j
$
EM
アルゴリズム
#
Word
Classes
意味
的
構造
的
語
語
こと
`(
Thursday
,
Friday
)`
vocabulary
`
V
`,
classes
`
C
`
語
`
w
`
class
`
c
`
写像
`
pi
`
```
python
pi
(
w
)
=
c
```
##
n
-
gram
class
model
写像
`
pi
`
上
クラス
n
-
gram
model
次
よう
```
python
Pr
(
w
[
k
]
|
w
[
1
:
k
-
1
])
=
Pr
(
w
[
k
]
|
c
[
k
])
Pr
(
c
[
k
]
|
c
[
1
:
k
-
1
])
```
ここ
n
-
gram
以上
```
python
Pr
(
c
[
k
]
|
c
[
1
:
k
-
1
])
=
Pr
(
c
[
k
]
|
c
[
k
-
n
+
1
:
k
-
1
])
```
training
text
右辺
2
確率
尤
推定
簡単
1
-
gram
場合
```
python
Pr
(
w
|
c
)
=
C
(
w
)
/
C
(
c
)
Pr
(
c
)
=
C
(
c
)
/
T
```
ここ
`
T
`
training
text
中
word
数
training
text
`
t
[
1
:
T
]`
`
C
(
c
)`
`
length
(
map
(
pi
,
t
))`
2
-
gram
$
Pr
(
c
[
2
]
|
c
[
1
])
=
\
dfrac
{
C
(
c
[
1
]
c
[
2
])}{\
sum
_
c
C
(
c
[
1
]
c
)}$
##
尤
度
$
L
(
pi
)
=
(
T
-
1
)^{-
1
}
\
log
Pr
(
t
[
2
:
T
]
|
t
[
1
])$
これ
尤
度
2
-
gram
model
下
これ
式
変形
$
L
(
pi
)
=
\
sum
_{
w
_
1
,
w
_
2
}
\
dfrac
{
C
(
w
_
1
w
_
2
)}{
T
-
1
}
\
log
Pr
(
c
_
2
|
c
_
1
)
Pr
(
w
_
2
|
c
_
2
)$
$
L
(
pi
)
=
-
H
(
w
)
+
I
(
c
1
,
c
2
)$
ここ
`
H
`
エントロピー
`
I
`
相互
情報
量
`
w
`
training
text
`
L
(
pi
)`
最大
化
よう
`
pi
`
選択
の
相互
情報
量
最大
化
よう
クラス
分類
選択
こと
##
Prictical
We
know
of
no
practical
method
to
find
max
`
I
`,
or
the
`
I
`
is
the
maximum
or
not
.
###
greedy
algorithm
-
goal
:
classifying
`
V
`
words
into
`
C
`
classes
(`
V
>
C
`)
-
initially
,
distincts
words
to
each
classes
,
that
is
there
are
`
V
`
classes
-
do
class
merge
`
V
-
C
`
times
(
in
a
step
,
one
merge
be
done
)
-
Then
,
we
get
`
C
`
classes
remained
The
step
is
described
recursively
as
follows
.
After
`
V
-
k
`
merges
,
we
got
`
k
`
classes
```
C
_
k
(
1
),
C
_
k
(
2
),
...,
C
_
k
(
k
)
```
we
think
of
the
merge
of
`
C
_
k
(
i
)`
with
`
C
_
k
(
j
)`
where
`
1
<=
i
<
j
<=
k
`.
```
python
p
_
k
(
l
,
m
)
=
Pr
(
C
_
k
(
l
),
C
_
k
(
m
))
```
This
is
the
probability
of
that
the
class
`
C
_
k
(
m
)`
follows
after
the
class
`
C
_
k
(
l
)`
in
the
text
.
let
$
pl
_
k
(
l
)
=
\
sum
_
m
p
_
k
(
l
,
m
)$
let
$
pr
_
k
(
m
)
=
\
sum
_
l
p
_
k
(
l
,
m
)$
let
$
q
_
k
(
l
,
m
)
=
p
_
k
(
l
,
m
)
\
log
\
dfrac
{
p
_
k
(
l
,
m
)}{
pl
_
k
(
l
)
pr
_
k
(
m
)}$
The
mutual
information
of
the
`
k
`
classes
is
denoted
by
$
I
_
k
=
\
sum
_{
l
,
m
}
q
_
k
(
l
,
m
)$
The
new
class
merged
`
C
_
k
(
i
)`
and
`
C
_
k
(
j
)`
is
denoted
by
`
i
+
j
`.
$
p
_
k
(
i
+
j
,
m
)
=
p
_
k
(
i
,
m
)
+
p
_
k
(
j
,
m
)$
$
q
_
k
(
i
+
j
,
m
)
=
p
_
k
(
i
+
j
,
m
)
\
log
\
dfrac
{
p
_
k
(
i
+
j
,
m
)}{
pl
_
k
(
i
+
j
)
pr
_
k
(
m
)}$
and
the
mutual
information
after
the
merge
is
$
I
_
k
(
i
,
j
)
=
I
_
k
-
s
_
k
(
i
)
-
s
_
k
(
j
)
+
q
_
k
(
i
,
j
)
+
q
_
k
(
j
,
i
)
+
q
_
k
(
i
+
j
,
i
+
j
)
+
\
sum
_{
l
\
ne
i
,
j
}
q
_
k
(
l
,
i
+
j
)
+
\
sum
_{
m
\
ne
i
,
j
}
q
_
k
(
i
+
j
,
m
)$
where
$
s
_
k
(
i
)
=
\
sum
_
l
q
_
k
(
l
,
i
)
+
\
sum
_
m
q
_
k
(
i
,
m
)
-
q
_
k
(
i
,
i
)$
So
,
we
will
find
the
pair
`(
i
,
j
)`
such
that
the
mutual
information
loss
$
L
_
k
(
i
,
j
)
=
I
_
k
-
I
_
k
(
i
,
j
)$
is
least
.
###
Classes
gotten
with
this
alogrithm
-
Friday
,
Monday
,
...
Sunday
,
weekends
-
June
,
March
,
July
...
-
people
guys
folks
fellows
...
-
down
,
backwards
,
ashore
,
sideways
...
-
water
,
gas
,
coal
,
liquid
...
-
had
,
hadn
'
t
hath
would
'
ve
could
'
ve
...
