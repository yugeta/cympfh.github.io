<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <link rel="stylesheet" href="../../css/m.css" type="text/css" />
</head>
<body>
<h1 id="learning-phrase-patterns-for-text-classification">Learning Phrase Patterns for Text Classification</h1>
<h2 id="intro">Intro</h2>
<p>テキスト分類。 N-gramなんかで十分な精度はある。 けれどもドメインに特化してしまうこと、applicationに依存してしまうことから 一般性がない。 n-gramに補充する素性の一つとして、phrase patternを使いたい。</p>
<h2 id="先行">先行</h2>
<h3 id="jaillet-2006">Jaillet+, 2006</h3>
<p>topic categorization</p>
<h3 id="wiebe-2005">Wiebe+, 2005</h3>
<p>文章の一人称を教師ナシで これは目的語を含んだフレーズパターンで分類した。 先行研究よりも高い精度。</p>
<h3 id="sun-2007">Sun+, 2007</h3>
<p>第二外国語学習者の書いた誤文法を検出。</p>
<h3 id="thur-2010-and-davidov-2010">Thur+, 2010 and Davidov+, 2010</h3>
<p>TwitterやAmazonレビューから「皮肉」な文を検出</p>
<h3 id="zhang-2010">Zhang+, 2010</h3>
<p>memo/N10-1108.md</p>
<p>で紹介したとおり、 N-gram よりも高精度であった！（まだ読んでない）</p>
<h1 id="手法">手法</h1>
<h2 id="phrase-pattern">phrase pattern</h2>
<p>普通パターンと言う場合の素のやつと、拡張バージョンのパターンの定義と、 パターンが文にマッチする、の定義を述べる。</p>
<h3 id="素-phrase-pattern">素 phrase pattern</h3>
<p>文をwordの列 <code>[u1, u2 .. u[n]]</code> とみなすのと全く同様に、 phrase pattern もまた、word の列 <code>[w1, w2 .. w[m]]</code> とする。 これが、先の文にマッチするとは、sub sequence になっていること。</p>
<p>i.e. <code>haskell sentence = [u1, u2 .. u[n]] pattern = [w1, w2 .. w[m]] n &gt;= m exist j = [j1, j2 .. j[m]] i1 &lt; i2 =&gt; j[i1] &lt; j[i2] forall i. w[i] == u[j[i]]</code></p>
<h3 id="extend">extend</h3>
<p>語の列でなく、word class も利用したい。 word class としては、POSとかpolarity とかを使い制限しない。 word <code>w</code> の class として(文脈に依存して) <code>{c1 .. cn}</code> があるとき、</p>
<pre><code>w -&gt; {w, c1 .. cn}</code></pre>
<p>という拡張を考える。 文と、phrase pattern ともに、この拡張を適用できる。 また、マッチすることの定義は、 最後の <code>==</code> を、 <code>subset</code> にする。</p>
<p>i.e. <code>haskell sentence = [u1, u2 .. u[n]] pattern = [w1, w2 .. w[m]] n &gt;= m exist j = [j1, j2 .. j[m]] i1 &lt; i2 =&gt; j[i1] &lt; j[i2] forall i. w[i] `subset` u[j[i]]</code></p>
<h2 id="learning">learning</h2>
<p>拡張バージョンの phrase pattern を学習するアルゴリズムを一つ挙げる。 他には CloSpan っていうのもある。</p>
<h3 id="prefixspan-pei-han-2001">PrefixSpan (Pei, Han+, 2001)</h3>
<p>コーパス <code>D</code> (文の集合) から、閾値 <code>f</code> 以上の頻度のあるような 素 phrase pattern を得る.</p>
<p>python-like コードで以下に示すが、これは、 prefix <code>rho</code> について、 先頭から再帰的に付け足すような再帰でパターンを得る。 <code>rho = []</code> からスタートする。</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> PrefixSpan(D, rho, f):
  P = [] <span class="co"># pattern set as return value</span>
  A = [] <span class="co"># new patterns</span>

  for_each a in D: <span class="co"># a is a token</span>
    rho<span class="st">&#39; = append(rho, a)</span>
<span class="st">    A.append(rho&#39;</span>) <span class="kw">if</span> match_freq(D, p) &gt;= f
  for_each a in D: <span class="co"># a is a token</span>
    rho<span class="st">&#39; = assemble(rho, a)</span>
<span class="st">    A.append(rho&#39;</span>) <span class="kw">if</span> match_freq(D, p) &gt;= f

  P = P.extend(A)

  for_each rho<span class="st">&#39; in A:</span>
<span class="st">    D&#39;</span> = project(D, rho<span class="st">&#39;)</span>
<span class="st">    B = PrefixSpan(D&#39;</span>, rho<span class="st">&#39;, f)</span>
<span class="st">    P = P.extend(B)</span>

<span class="st">  return P</span></code></pre>
<p><code>match_freq</code> はマッチする回数。</p>
<p><code>append</code> および <code>assemble</code> はパターンとトークンから新しいパターンを作る。 ```python def append(rho, a): return rho.append({a})</p>
<p>def assemble(rho, a): init = rho[0:-1] last = rho[-1] return init.append(last.union({a})) ```</p>
<p>言い直すと、</p>
<p>パターン <code>[{w1,c1..c1} .. {wn,cn..cn}]</code> に <code>s</code> をつけたす方法として、</p>
<ul class="incremental">
<li><code>[{w1,c1..c1} .. {wn,cn..cn}, {s}]</code> # append</li>
<li><code>[{w1,c1..c1} .. {wn,cn..cn, s}]</code> # assemble</li>
</ul>
<p>がある、って言ってる。</p>
<p>あと、コーパスDについてのパターンrhoによるproject とは、 マッチする文だけフィルタリングしたもの。</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> Project(D, rho):
  D<span class="st">&#39; = []</span>
<span class="st">  for_each s in D:</span>
<span class="st">    D&#39;</span> = D<span class="st">&#39;.append(s) if match(rho, s)</span>

<span class="st">  return D&#39;</span></code></pre>
<p>論文に載ってるのはもうちょっと複雑で、 効率よく動くようになってる。 上の場合、 パターン <code>[w1]</code> を採択したら、 <code>project</code> で それにマッチする文だけを集めて、 次は <code>[w1, w2]</code> を見る。</p>
<p>しかし、 先頭から作って行くのだから、 <code>project</code> の定義を、マッチした文、じゃなくて、マッチした文のマッチより後ろ部分だけ、にすることで、 <code>[w2]</code>を見ればよくなる。</p>
<h2 id="尺度">尺度</h2>
<p>先のアルゴリズムでは、頻度という尺度で、 パターンを採択するか決めていたが、 相互情報量のほうがいいだろう。</p>
<p>コーパスD に、クラス <code>Y = 1, 2 .. K</code> があって、 変数<code>X</code>はあるパターンがマッチするかどうか(<code>X=0,1</code>) とすると、</p>
<p>定義通りには、 <code>haskell I X Y = sum [ sum [ (p x y) * log ((p x y) / (p x) / (p y))  | y &lt;- [1..K] ] | x &lt;- [0,1] ]</code></p>
<p>次のように書き換えると、計算しよい。 (計算効率をおとさない) <code>haskell p_given x y -- probability of x given y I X Y = sum [ sum [ (p_given x y) * (p y) * log ((p_given x y) / p_x) | y &lt;- [1..K] ] | x &lt;- [0,1] ]   where     p_x = sum [ p_given x y' * p y' | y' &lt;- [1 .. K] ]</code></p>
<p>で、えっと、あるパターン <code>p</code> について の、<code>X</code>に対して そのパターンを拡張した時のそれを <code>XE</code>と書くと、 <code>p(XE=1|y) &lt;= p(X=1|y)</code> が当然なりたつ。 したがって、相互情報量の上限 <code>sup_E I(XE;Y)</code> が存在する。 (まじで？？？)</p>
<p>改訂版のアルゴリズム <code>ExtendedPrefixSpan</code></p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> ExtendedPrefixSpan(D, rho, Theta):
  P = []

  for_each t in D:
    rho<span class="st">&#39; = rho.append(t)</span>
<span class="st">    rho&#39;</span> の相互情<span class="dv">報</span>量 が閾値以上なら、
      P = P.append(rho<span class="st">&#39;)</span>

<span class="st">    else 上限が閾値を超えるなら</span>
<span class="st">      D&#39;</span> = project(D, rho<span class="st">&#39;)</span>
<span class="st">      P&#39;</span> = ExtendedPrefixSpan(D<span class="st">&#39;, rho&#39;</span>, Theta)
      P = P.extend(P<span class="st">&#39;)</span>

<span class="st">  return P</span></code></pre>
<h1 id="word-classes">Word Classes</h1>
<h2 id="lemma">Lemma</h2>
<p>canonical form of a word のこと。 <code>haskell {go, goes, going, went gone} -&gt; go</code></p>
<p>NLTK WordNet lemmatizer を使う</p>
<h2 id="word-shape">Word shape</h2>
<p>大文字の使われ方。 全部大文字になってるか、最初だけか、ドットで連結した大文字（つまり省略形）か。</p>
<h2 id="pos">POS</h2>
<p>いわずもがな。 Stanford log-lenear POS tagger ってのがあって、 英語と中国語に対して使える？らしいよ。</p>
<h2 id="named-entity-ne">Named entity (NE)</h2>
<p>テキスト分類についてはこれはすごい大事な素性。</p>
<pre><code>(sentence, word) -&gt; class ({Location, Person, Organization, Time, Date})</code></pre>
<p>Stanford conditional random field-based NE recognizer (NER) なるものが良いって。</p>
<h2 id="liwc-dictionary-89.95">LIWC dictionary ($89.95)</h2>
<p>Linguistic Inquiry and Word Count (LIWC) は、単語を64の(感情の?)クラスに分類する。 Facebookが使った奴。 文脈に依存せず、一つの単語について分析する。</p>
<p><a href="http://www.liwc.net/tryonline.php">LIWC: Linguistic Inquiry and Word Count</a></p>
<h2 id="mpqa-subjectivity-lexicon">MPQA subjectivity lexicon</h2>
<p>under GNU GPL</p>
<p>自己申告で個人情報送ると即座にダウンロードできる。 <a href="http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/">Subjectivity Lexicon | MPQA</a></p>
<pre><code>(word, POS) -&gt; class</code></pre>
<p>8222 (word,POS) 登録されてる。</p>
<pre><code>type=weaksubj len=1 word1=dominate pos1=verb stemmed1=y priorpolarity=negative</code></pre>
<h2 id="manual">manual</h2>
<blockquote>
<p>we use various word listsc onstructed by linguists who have looked at data related to some of our tasks.</p>
</blockquote>
<p>つまり手作業で、 あるクラスに属する単語のリストを作る． これは、タスクごとに、そのトピックに詳しい人間がやる．</p>
<p>例えば、後の実験で使った例では、</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="dt">AGREEMENT</span> <span class="fu">=</span> [right, agree, true]
<span class="dt">DISAGREEMENT</span> <span class="fu">=</span> [doubt, inappropriate]
<span class="dt">ALIGNMENT</span> <span class="fu">=</span> <span class="dt">AGREEMENT</span> <span class="fu">++</span> <span class="dt">DISAGREEMENT</span>
<span class="dt">MODAL</span> <span class="fu">=</span> [could, should]
<span class="dt">NEGATIVE_DISCOURSE_ORDER</span> <span class="fu">=</span> [however, but, nevertheless]</code></pre>
<h2 id="automatic">automatic</h2>
<p>1次マルコフモデルを使って、 wordをクラスタリングする． クラスた数は 10, 100, 1000 ってする．</p>
<p>Brown+, 1992 &quot;Class-based n-gram models of natural language&quot;</p>
<h1 id="実験">実験</h1>
<p>n-gramと他の素性を使えば十分に分類可能なタスクは してもしょうがないので、 それなりに難しいタスクを３つやる．</p>
<ul class="incremental">
<li>speaker role</li>
<li>alignment move</li>
<li>authority claim</li>
</ul>
<p>初めに訓練データでパターンを学習して、 n-gramの場合と、パターンの場合を比較する． 公平のために、n-gramは3-gramまで、 パターンも長さ3までにする．</p>
<ul class="incremental">
<li>Maximum entropy classification
<ul class="incremental">
<li><code>P(c|d) = 1/(Z d) * exp (sum_i [ lambda_i * f_i ])</code></li>
</ul></li>
<li>5-fold cross validation</li>
</ul>
<h2 id="speaker-role">Speaker role</h2>
<p>ニュースショーにおける、(人, その人が発した言葉) から、 その人のショーにおける役割をあてる． 役割とは、Host, Guest, Voice bite</p>
<p>Liu+ 80%</p>
<p>48 English talks and 90 Mandarin talks の録音に対して、 REF (Reference human transcripts) と ASR (automatic speech recognition) output (using SRI Decipher ASR system) を対象にする．</p>
<p>ASR は、結構間違えることに註意． 英語については22.8% 北京語については38.6% くらい、単語/文字を誤る．</p>
<p>kappa = 0.67 / 0.78</p>
<h2 id="alignment-move">Alignment move</h2>
<p>ネット上の議論において、 参加者の同意(positive)、異論(negative) を見る． 文に対して、 pos/neg をつける． あるアノテータがposをつけて、あるアノテータがnegを つけたら pos+neg というラベルにする．</p>
<p>実験で使うのは Wikipedia talk page</p>
<p>211 English pages and 225 Chinese pages</p>
<p>kappa = 0.50 / 0.53</p>
<p>で、たまに pos/neg 両方を含むような面倒な文がある． そこで、 pos/none, neg/none の２つの分類器を作って</p>
<h2 id="authority-claim">Authority claim</h2>
<blockquote>
<p>showing her knowledge or experience with respect to a topic, or using some external evidence to support herself</p>
</blockquote>
<p>Marin+, 2010: Unigramで実際けっこういい． (外のページヘの引用とかがある)</p>
<p>339 English pages and 225 Chinese pages</p>
<p>発言ごとに、authority claimであるかどうか．</p>
<p>kappa = 0.59 / 0.73</p>
<p>全体の20%がauthority claim であった．</p>
<h2 id="result">Result</h2>
<p>表は適宜参照のこと． ここには書くことはしない．</p>
<p>Table I は、PrefixSpan と、ExtendedPrefixSpan との差を見る． -0.2%から+4.1%とか．</p>
<p>Table IIからVIIは、baseline (with only n-gram) と、pattern (with each class) との比較．</p>
<p>Speech role は、REFでもASRでも十分な結果が得られている． つまり、頑強である．</p>
<p>manualは利用ならば、それが最強</p>
<p>Wikipedia English pages alignmentについてのパターンとして、 <code>i ALIGNMENT MODAL a POSITIVE #idea</code> とか．</p>
<p>あ、そうそう．英語のパターンの場合は、 ２つのトークンが連続で出現してなくてもマッチするわけだけど、 上のように<code>#</code>というのは、連続であることを意味するらしい． 初めからそうすればいいのにね．</p>
</body>
</html>
