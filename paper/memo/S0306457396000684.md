# シソーラスの構築

1995年の論文。
共起ベクトルで単語を表現する手法は既にあったけど、
それをシソーラスの構築に役立てたのは、これが初めてっぽい。

ここでいうシソーラスの構築ってのは、
類語辞典のことを指してる。

`second-order cooccurrence` なるものを提案する

## 関連

### 歴史

一番古くて、単純で、しかし確実なのは手で作る方法 (`sentiment mapping`)だろう。
しかし、その道のプロでないとこれはできない。
つまり、専門用語の意味が分からないとできない。

既にあるオンライン辞書データベース (WordNet, Longman's subject codes)
を利用する方法。
しかし、これは特定の分野に特化することができなかった。
たとえば法律用語について調べたくてもなんか一般的なのばっかりが出てくる？みたいな？

それに対する方法はもちろん、コーパスを利用する方法である。
階層的シソーラス [Evans+, 1991]
は、ある語が他のある語を包摂 (subsumption) してるという関係を表現したもの。
(e.g. `intelligence` subsume `artificial intelligence`)

頭で修飾する語 (head-modifier) の関係で意味的な近さを決定する手法
[Grefenstette, 1992; Ruge, 1992; Strzalkowski, 1995]
だんだん、共起に近づいて来た。
この手法だと、`doctor` と `disease` の近さが取れる。
でも `Japanese/Chilean government` という文から
`Japanese` と `Chilean` が近いとされるけど、
それは微妙だという指摘。

`Tokyo`, `Andes`, `Samurai` などという語との共起から
日本の話なのかチリの話なのかが指摘できる、ということが、
その著者の内の Grefenstette, Strzalkowski が言ってた。
じゃあ共起でいいよな。

Crouch (1990) が共起ベクトルを考えるようになったらしいぞ。
これによって、documents をクラスタリングすることなどをした。
ただ、本当の同義語は共起しないだろいう、という直感がある。

Peat and Willet (1991) は word `x` と word `y` の近さに、
それらが出現する document の数 `f-x`, `f-y`
と、同時に出現する document の数 `f-xy` を用いて

```scheme
(/ f-xy (sqrt (* f-x f-y)))
```

とした。

これらが見る共起を first-order と言ってる。
つまり、 `words sharing the same neightbords`
である、から先ほどの直感が正しく当たっていて、
本人らが言うに、
`accident` (freq. 2590)
と
`mishaps` (freq. 129)
の意味の近さが取れない。

そこで、この論文では二次 (second-order) の共起を見る。
言ってしまうと、
直で周辺に同じような語があるか？が一次であり、
周辺の同じような語の周辺に同じような語があるか？が二次である。
二次だと、同義語も得やすそうだろう。

### Latent Semantic Indexing; LSI

歴史的な経緯とはまた別に、形式的なこと。

`document-by-word` 行列 の特異値分解(SVD)

今回は、
`term-by-document` 行列をSVDする。

`A` を `word-by-document` matrix
とする。
これのSVDは

```haskell
A = U * S * V'
```

ここで

- `U` and `V` は直交行列 (`U * U' = I`)
- `S`があの固有値の対格行列

```haskell
C = A * A'
```

を考えると、これは `word-by-word` matrix である。
すなわち、

`C_{i,j}` は word `i` と `j` のdocument単位での共起頻度になる。

ちょっと簡約してみると、

```haskell
C = A * A' = (USV') * (USV')’ = U * S^2 & U'
```

これは、いままで document を単位で共起を見ていた。
すなわち context を document としていた。
提案手法では `k` words を context とする。

### 共起頻度シソーラス

`term-by-term matrix` `C` をまず計算することから始める。
すなわち、
`C_{i, j}` は `k` words ウィンドウサイズ中の word `i` と `j` の共起頻度である。

```haskell
let k = 40
```

語彙数が v なら C は v<sup>2</sup> 行列であって、要素の半分が異なるものを意味する。
この v は大きくなることが予想される。

なんか、SVDの計算コストは O(v<sup>2</sup>) らしい。

というわけで、
SVDを使う以上、それに入れる行列はある程度小さくしたい、
という要望がある。

## Method

### Word-base

1. stop word は適宜抜く。
1. そこそこの頻度の 3000 A-words を得る
1. 3000 * 3000 の共起頻度行列 (Matrix A) (k=40 words a window)
1. cos距離で 200 A-class にクラスタリング
1. A-class ごとに A-word の周辺に出現した B-word を集めることで 20000 B-words * A-class の行列 (Matrix B) を得る
1. 200 B-classes にクラスタリングをするが、数が多いので Buckshot fast clustering algorithm (Cutting+, 1992) を適用する
1. コーパスにある全部の単語に対して B-word (in B-classes) の近くに出現 (within a window) した頻度で、 words * B-classes の行列 (Matrix C') を得る
1. C' に対して SVM して C を得る (20 C-lasses)
1. exit

### Context vectors

途中

